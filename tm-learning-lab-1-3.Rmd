---
title: "Public Sentiment & the State Standards"
subtitle: "Text Mining Learning Lab  1.3"
author: "LASER Institute"
date: "2/21/2021"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

This week, our walkthrough is guided by my colleague Josh Rosenberg's recent article, **Advancing new methods for understanding public sentiment about educational reforms: The case of Twitter and the Next Generation Science Standards.** We will focus on conducting a very simplistic "replication study" by comparing the sentiment of tweets about the [Next Generation Science Standards](https://www.nextgenscience.org) (NGSS) and [Common Core State Standards](http://www.corestandards.org) (CCSS) in order to better understand public reaction to these two curriculum reform efforts.

### Tidy Text Flow Chart

For Unit 2, our focus will be on using the Twitter API to import data on topics or tweets of interest and using sentiment lexicons to help gauge public opinion about those topics or tweets. Silge & Robinson nicely illustrate the tools of text mining to approach the emotional content of text programmatically, in the following diagram:

![Figure 2.1: A flowchart of a typical text analysis that uses tidytext for sentiment analysis.](img/sentiment-flow.png "Figure 2.1: A flowchart of a typical text analysis that uses tidytext for sentiment analysis. This chapter shows how to implement sentiment analysis using tidy data principles."){width="90%"}

For Unit 2, our walkthrough will cover the following topics:

1.  **Prepare**: Prior to analysis, it's critical to understand the context and data sources you're working with so you can formulate useful and answerable questions. We'll take a quick look at Dr. Rosenberg's study as well as data available through Twitter's API.
2.  **Wrangle**: In section 2 we revisit tidying and tokenizing text from Unit 1, and and learn some new functions for appending sentiment scores to our tweets using the AFFIN, bing, and nrc sentiment lexicons.
3.  **Explore**: In section 3, we use simple summary statistics and basic data visualization to compare sentiment between NGSS and CCSS tweets.
4.  **Model**: While we won't leverage modeling approaches until Unit 3, we will examine the mixed effects model used by Rosenberg et al. to analyze the sentiment of tweets

### 1a. Some Context

### 1b. Guiding Questions

For this walkthrough, we'll use a similar approach used by the authors to guage public sentiment around the NGSS, by compare how much more positive or negative NGSS tweets are relative to CSSS tweets.

Our (very) specific questions of interest for this walkthrough are:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for NGSS compare to sentiment for CCSS?
3.  How does public sentiment vary over time?

And just to reiterate from Learning Lab 1, one overarching question we'll explore throughout this module, and that Silge and Robinson (2018) identify as a central question to text mining and natural language processing, is:

> How do we to **quantify** what a document or collection of documents is about?

```{r}
library(dplyr)
library(readr)
library(tidyr)
library(rtweet)
library(writexl)
library(readxl)
library(tidytext)
library(textdata)
library(ggplot2)
library(textdata)
library(scales)
```

------------------------------------------------------------------------

## 2. WRANGLE

a.  

b.  **Get Sentiments**. We conclude our data wrangling by introducing sentiment lexicons and the `inner_join()` function for appending sentiment values to our data frame.

### 2a. Add Sentiment Values

Now that we have our tweets nice and tidy, we're almost ready to begin exploring public sentiment (at least for the past week due to Twitter API rate limits) around the CCSS and NGSS standards. For this part of our workflow we introduce two new functions from the [`tidytext`](https://github.com/juliasilge/tidytext) and `dplyr` packages respectively:

-   `get_sentiments()` returns specific sentiment lexicons with the associated measures for each word in the lexicon
-   `inner_join()` return all rows from `x` where there are matching values in `y`, and all columns from `x` and `y`.

For a quick overview of the different join functions with helpful visuals, visit: <https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti>

#### Get Sentiments

Recall from our readings that sentiment analysis tries to evaluate words for their emotional association. Silge & Robinson point out that, "one way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words." As our readings from last week illustrated, this isn't the only way to approach sentiment analysis, but it is an easier entry point into sentiment analysis and often-used.

The tidytext package provides access to several sentiment lexicons based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth.

The three general-purpose lexicons we'll focus on are:

-   `AFINN` assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

-   `bing` categorizes words in a binary fashion into positive and negative categories.

-   `nrc` categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

Note that if this is your first time using the AFINN and NRC lexicons, you'll be prompted to download both Respond yes to the prompt by entering "1" and the NRC and AFINN lexicons will download. You'll only have to do this the first time you use the NRC lexicon.

Let's take a quick look at each of these lexicons using the `get_sentiments()` function and assign them to their respective names for later use:

```{r}
afinn <- get_sentiments("afinn")

afinn
```

```{r}
bing <- get_sentiments("bing")

bing
```

```{r}
nrc <- get_sentiments("nrc")

nrc
```

And just out of curiosity, let's take a look at the `loughran` lexicon as well:

```{r}
loughran <- get_sentiments("loughran")

loughran
```

##### ✅ Comprehension Check

1.  How were these sentiment lexicons put together and validated? Hint: take a look at Chapter 2 from Text Mining with R.
2.  Why should we be cautious when using and interpreting them?

#### Join Sentiments

We've reached the final step in our data wrangling process before we can begin exploring our data to address our questions.

In the previous section, we used `anti_join()` to remove stop words in our dataset. For sentiment analysis, we're going use the `inner_join()` function to do something similar. However, instead of removing rows that contain words matching those in our stop words dictionary, `inner_join()` allows us to keep only the rows with words that match words in our sentiment lexicons, or dictionaries, along with the sentiment measure for that word from the sentiment lexicon.

Let's use `inner_join()` to combine our two `tidy_tweets` and `afinn` data frames, keeping only rows with matching data in the `word` column:

```{r}
sentiment_afinn <- inner_join(tidy_tweets, afinn, by = "word")

sentiment_afinn
```

Notice that each word in your `sentiment_afinn` data frame now contains a value ranging from -5 (very negative) to 5 (very positive).

```{r}
sentiment_bing <- inner_join(tidy_tweets, bing, by = "word")

sentiment_bing
```

```{r, echo=F, results=F}
sentiment_nrc <- inner_join(tidy_tweets, nrc, by = "word")

sentiment_nrc

sentiment_loughran <- inner_join(tidy_tweets, loughran, by = "word")

sentiment_loughran
```

##### ✅ Comprehension Check

1.  Create a `sentiment_nrc` data frame using the code above.
2.  What do you notice about the change in the number of observations (i.e. words) between the `tidy_tweets` and data frames with sentiment values attached? Why did this happen?

**Note:** To complete to the following section, you'll need the `sentiment_nrc` data frame.

## 3. EXPLORE

Now that we have our tweets tidied and sentiments joined, we're ready for a little data exploration. As highlighted in Unit 1, calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. One goal in this phase is explore questions that drove the original analysis and develop new questions and hypotheses to test in later stages. Topics addressed in Section 3 include:

a.  **Time Series**. We take a quick look at the date range of our tweets and compare number of postings by standards.
b.  **Sentiment Summaries**. We put together some basic summaries of our sentiment values in order to compare public sentiment

### 3a. Time Series

Before we dig into sentiment, let's use the handy `ts_plot` function built into `rtweet` to take a very quick look at how far back our tidied `tweets` data set goes:

```{r}
ts_plot(tweets, by = "days")
```

Notice that this effectively creates a `ggplot` time series plot for us. I've included the `by =` argument which by default is set to "days". It looks like tweets go back 9 days which the rate limit set by Twitter.

Try changing it to "hours" and see what happens.

##### ✅ Comprehension Check

1.  Use `ts_plot` with the `group_by` function to compare the number of tweets over time by Next Gen and Common Core `standards`
2.  Which set of standards is Twitter users talking about the most?

Hint: use the `?ts_plot` help function to check the examples to see how this can be done.

Your line graph should look something like this:

```{r, echo=F}
tweets %>%
  group_by(standards) %>%
  ts_plot(by = "days")
```

### 3b. Sentiment Summaries

Since our primary goals is to compare public sentiment around the NGSS and CCSS state standards, in this section we put together some basic numerical summaries using our different lexicons to see whether tweets are generally more positive or negative for each standard as well as differences between the two. To do this, we revisit the following `dplyr` functions:

-   [`count()`](https://dplyr.tidyverse.org/reference/count.html?q=count) lets you quickly count the unique values of one or more variables

-   [`group_by()`](https://dplyr.tidyverse.org/articles/grouping.html?q=group) takes a data frame and one or more variables to group by

-   [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) creates a numerical summary of data using arguments like [`mean()`](https://rdrr.io/r/base/mean.html) and [`median()`](https://rdrr.io/r/stats/median.html)

-   [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) adds new variables and preserves existing ones

And introduce one new function:

-   `spread()`

#### Sentiment Counts

Let's start with `bing`, our simplest sentiment lexicon, and use the `count` function to count how many times in our `sentiment_bing` data frame "positive" and "negative" occur in `sentiment` column and :

```{r}
summary_bing <- count(sentiment_bing, sentiment, sort = TRUE)
```

Collectively, it looks like our combined dataset has more positive words than negative words.

```{r}
summary_bing
```

Since our main goal is to compare positive and negative sentiment between CCSS and NGSS, let's use the `group_by` function again to get `sentiment` summaries for NGSS and CCSS separately:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment) 

summary_bing
```

Looks like CCSS have far more negative words than positive, while NGSS skews much more positive. So far, pretty consistent with Rosenberg et al. findings!!!

#### Compute Sentiment Value

Our last step will be calculate a single sentiment "score" for our tweets that we can use for quick comparison and create a new variable indicating which lexicon we used.

First, let's untidy our data a little by using the `spread` function from the `tidyr` package to transform our `sentiment` column into separate columns for `negative` and `positive` that contains the `n` counts for each:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) 

summary_bing
```

Finally, we'll use the `mutate` function to create two new variables: `sentiment` and `lexicon` so we have a single sentiment score and the lexicon from which it was derived:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(lexicon = "bing") %>%
  relocate(lexicon)

summary_bing
```

There we go, now we can see that CCSS scores negative, while NGSS is overall positive.

Let's calculate a quick score for using the `afinn` lexicon now. Remember that AFINN provides a value from -5 to 5 for each:

```{r}
head(sentiment_afinn)
```

To calculate late a summary score, we will need to first group our data by `standards` again and then use the `summarise` function to create a new `sentiment` variable by adding all the positive and negative scores in the `value` column:

```{r}
summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```

Again, CCSS is overall negative while NGSS is overall positive!

##### ✅ Comprehension Check

For your final task for this walkthough, calculate a single sentiment score for NGSS and CCSS using the remaining `nrc` and `loughan` lexicons and answer the following questions. Are these findings above still consistent?

Hint: The `nrc` lexicon contains "positive" and "negative" values just like `bing` and `loughan`, but also includes values like "trust" and "sadness" as shown below. You will need to use the `filter()` function to select rows that only contain "positive" and "negative."

```{r}
nrc
```

```{r, echo=F, message=F}
summary_nrc <- sentiment_nrc %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "nrc")  %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive/negative)

summary_nrc

summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>%
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```

## 4. MODEL

As highlighted in [Chapter 3 of Data Science in Education Using R](https://datascienceineducation.com/c03.html), the **Model** step of the data science process entails "using statistical models, from simple to complex, to understand trends and patterns in the data." The authors note that while descriptive statistics and data visualization during the **Explore** step can help us to identify patterns and relationships in our data, statistical models can be used to help us determine if relationships, patterns and trends are actually meaningful.

Recall from the PREPARE section that the Rosenberg et al. study was guide by the following questions:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for teachers differ from non-teachers?
3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?
4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?
5.  How does public sentiment vary over time?

Similar to our sentiment summary using the AFINN lexicon, the Rosenberg et al. study used the -5 to 5 sentiment score from the SentiStrength lexicon to answer RQ \#1. To address the remaining questions the authors used a mixed effects model (also known as multi-level or hierarchical linear models via the lme4 package in R.

Collectively, the authors found that:

1.  The SentiStrength scale indicated an overall neutral sentiment for tweets about the Next Generation Science Standards.
2.  Teachers were more positive in their posts than other participants.
3.  Posts including \#NGSSchat that were posted outside of chats were slightly more positive relative to those that did not include the \#NGSSchat hashtag.
4.  The effect upon individuals of being involved in the \#NGSSchat was positive, suggesting that there is an impact on individuals---not tweets---of participating in a community focused on the NGSS.
5.  Posts about the NGSS became substantially more positive over time.

## 5. COMMUNICATE

The final(ish) step in our workflow/process is sharing the results of analysis with wider audience. Krumm et al. (2018) outlined the following 3-step process for communicating with education stakeholders what you have learned through analysis:

1.  **Select**. Communicating what one has learned involves selecting among those analyses that are most important and most useful to an intended audience, as well as selecting a form for displaying that information, such as a graph or table in static or interactive form, i.e. a "data product."
2.  **Polish**. After creating initial versions of data products, research teams often spend time refining or polishing them, by adding or editing titles, labels, and notations and by working with colors and shapes to highlight key points.
3.  **Narrate**. Writing a narrative to accompany the data products involves, at a minimum, pairing a data product with its related research question, describing how best to interpret the data product, and explaining the ways in which the data product helps answer the research question.

### 5a. Select

Remember that the questions of interest that we want to focus on our for our selection, polishing, and narration include:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for NGSS compare to sentiment for CCSS?

To address questions 1 and 2, I'm going to focus my analyses, data products and sharing format on the following:

1.  **Analyses**. For RQ1, I'm want to try and replicate as closely as possible the analysis by Rosenberg et al. so I will clean up my analysis and calculate a single sentiment score using the AFINN Lexicon for the entire tweet and label it positive or negative based on that score. I also want to highlight how regardless of the lexicon selected, NGSS tweets contain more positive words than negative, so I'll also polish my previous analyses and calculate percentages of positive and negative words for the
2.  **Data Products**. I know these are shunned in the world of data viz, but I think a **pie chart** will actually be an effective way to quickly communicate the proportion of positive and negative tweets among the Next Generation Science Standards. And for my analyses with the `bing`, `nrc`, and `loughan` lexicons, I'll create some 100% stacked bars showing the percentage of positive and negative words among all tweets for the NGSS and CCSS.
3.  **Format**. Similar to Unit 1, I'll be using [R Markdown](https://rmarkdown.rstudio.com) again to create a quick slide deck. Recall that R Markdown files can also be used to create a [wide range of outputs and formats](https://rmarkdown.rstudio.com/gallery.html), including polished PDF or Word documents, websites, web apps, journal articles, online books, interactive tutorials and more. And to make this process even more user-friendly, R Studio now includes [a visual editor](https://rstudio.github.io/visual-markdown-editing/#/)!

### 5b. Polish

#### NGSS Sentiment

I want to try and replicate as closely as possible the approach Rosenberg et al. used in their analysis. To do that, I'll I can recycle some R code I used in section [2b. Tidy Text].

To polish my analyses and prepare, first I need to rebuild the `tweets` dataset from my `ngss_tweets` and `ccss_tweets` and select both the `status_id` that is unique to each tweet, and the `text` column which contains the actual post:

```{r}
ngss_text <-
  ngss_tweets %>%
  filter(lang == "en") %>%
  select(status_id, text) %>%
  mutate(standards = "ngss") %>%
  relocate(standards)

ccss_text <-
  ccss_tweets %>%
  filter(lang == "en") %>%
  select(status_id, text) %>%
  mutate(standards = "ccss") %>%
  relocate(standards)

tweets <- bind_rows(ngss_text, ccss_text)

tweets
```

The `status_id` is important because like Rosenberg et al., I want to calculate an overall sentiment score for each tweet, rather than for each word.

Before I get that far however, I'll need to tidy my `tweets` again and attach my `sentiment` scores.

Note that the closest lexicon we have available in our `tidytext` package to the SentiStrength lexicon used by Rosenberg is the AFINN lexicon which also uses a -5 to 5 point scale.

So let's use `unnest_tokens` to tidy our tweets, remove stop words, and add `afinn` scores to each word similar to what we did in section [2c. Add Sentiment Values]:

```{r, message=F}
sentiment_afinn <- tweets %>%
  unnest_tokens(output = word, 
                input = text, 
                token = "tweets")  %>% 
  anti_join(stop_words, by = "word") %>%
  filter(!word == "amp") %>%
  inner_join(afinn, by = "word")

sentiment_afinn
```

Next, I want to calculate a single score for each tweet. To do that, I'll use the by now familiar `group_by` and `summarize`

```{r, message=F}
afinn_score <- sentiment_afinn %>% 
  group_by(standards, status_id) %>% 
  summarise(value = sum(value))

afinn_score
```

And like Rosenberg et al., I'll add a flag for whether the tweet is "positive" or "negative" using the `mutate` function to create a new `sentiment` column to indicate whether that tweets was positive or negative.

To do this, we introduced the new `if_else` function from the `dplyr` package. This `if_else` function adds "negative" to the `sentiment` column if the score in the `value` column of the corresponding row is less than 0. If not, it will add a "positive" to the row.

```{r}
afinn_sentiment <- afinn_score %>%
  filter(value != 0) %>%
  mutate(sentiment = if_else(value < 0, "negative", "positive"))

afinn_sentiment
```

Note that since a tweet sentiment score equal to 0 is neutral, I used the `filter` function to remove it from the dataset.

Finally, we're ready to compute our ratio. We'll use the `group_by` function and `count` the number of tweets for each of the `standards` that are positive or negative in the `sentiment` column. Then we'll use the `spread` function to separate them out into separate columns so we can perform a quick calculation to compute the `ratio`.

```{r}
afinn_ratio <- afinn_sentiment %>% 
  group_by(standards) %>% 
  count(sentiment) %>% 
  spread(sentiment, n) %>%
  mutate(ratio = negative/positive)

afinn_ratio
```

Finally,

```{r}
afinn_counts <- afinn_sentiment %>%
  group_by(standards) %>% 
  count(sentiment) %>%
  filter(standards == "ngss")

afinn_counts %>%
ggplot(aes(x="", y=n, fill=sentiment)) +
  geom_bar(width = .6, stat = "identity") +
  labs(title = "Next Gen Science Standards",
       subtitle = "Proportion of Positive & Negative Tweets") +
  coord_polar(theta = "y") +
  theme_void()

```

#### NGSS vs CCSS

Finally, to address Question 2, I want to compare the percentage of positive and negative words contained in the corpus of tweets for the NGSS and CCSS standards using the four different lexicons to see how sentiment compares based on lexicon used.

I'll begin by polishing my previous summaries and creating identical summaries for each lexicon that contains the following columns: `method`, `standards`, `sentiment`, and `n`, or word counts:

```{r}
summary_afinn2 <- sentiment_afinn %>% 
  group_by(standards) %>% 
  filter(value != 0) %>%
  mutate(sentiment = if_else(value < 0, "negative", "positive")) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "AFINN")

summary_bing2 <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "bing")

summary_nrc2 <- sentiment_nrc %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "nrc") 

summary_loughran2 <- sentiment_loughran %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "loughran") 
```

Next, I'll combine those four data frames together using the `bind_rows` function again:

```{r}
summary_sentiment <- bind_rows(summary_afinn2,
                               summary_bing2,
                               summary_nrc2,
                               summary_loughran2) %>%
  arrange(method, standards) %>%
  relocate(method)

summary_sentiment
```

Then I'll create a new data frame that has the total word counts for each set of standards and each method and join that to my `summary_sentiment` data frame:

```{r}
total_counts <- summary_sentiment %>%
  group_by(method, standards) %>%
  summarise(total = sum(n))

sentiment_counts <- left_join(summary_sentiment, total_counts)

sentiment_counts
```

Finally, I'll add a new row that calculates the percentage of positive and negative words for each set of state standards:

```{r}
sentiment_percents <- sentiment_counts %>%
  mutate(percent = n/total * 100)

sentiment_percents
```

Now that I have my sentiment percent summaries for each lexicon, I'm going great my 100% stacked bar charts for each lexicon:

```{r}
sentiment_percents %>%
  ggplot(aes(x = standards, y = percent, fill=sentiment)) +
  geom_bar(width = .8, stat = "identity") +
  facet_wrap(~method, ncol = 1) +
  coord_flip() +
  labs(title = "Public Sentiment on Twitter", 
       subtitle = "The Common Core & Next Gen Science Standards",
       x = "State Standards", 
       y = "Percentage of Words")
```

And finished! The chart above clearly illustrates that regardless of sentiment lexicon used, the NGSS contains more positive words than the CCSS lexicon.

### 5c. Narrate

With our "data products" cleanup complete, we can start pulling together a quick presentation to share with the class. We've already seen what a more formal journal article looks like in the PREPARE section of this walkthrough. For your Independent Analysis assignment for Unit 2, you'll be creating either a simple report or slide deck to share out some key findings from our analysis.

Regardless of whether you plan to talk us through your analysis and findings with a presentation or walk us through with a brief written report, your assignment should address the following questions:

1.  **Purpose**. What question or questions are guiding your analysis? What did you hope to learn by answering these questions and why should your audience care about your findings?
2.  **Methods**. What data did you selected for analysis? What steps did you take took to prepare your data for analysis and what techniques you used to analyze your data? These should be fairly explicit with your embedded code.
3.  **Findings**. What did you ultimately find? How do your "data products" help to illustrate these findings? What conclusions can you draw from your analysis?
4.  **Discussion**. What were some of the strengths and weaknesses of your analysis? How might your audience use this information? How might you revisit or improve upon this analysis in the future?

#### Examples and Templates

You can view my example presentation here: COMING SOON!

And use my R Markdown presentation file as a template: COMING SOON!
