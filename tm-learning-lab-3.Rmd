---
title: "Come to the Dark Side"
subtitle: "Text Mining Learning Lab 1.3"
author: "LASER Institute"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

Recall from TM Learning Lab 1 that one of our goals for these labs is to produce a simplistic replication of the analyses by Rosenberg et al (2021). Specifically, we want to compare the sentiment of tweets about the [Next Generation Science Standards](https://www.nextgenscience.org) (NGSS) and [Common Core State Standards](http://www.corestandards.org) (CCSS) in order to better understand public reaction to these two curriculum reform efforts.

In TM Learning Lab 3: Come to the Dark Side we focus on the use of lexicons for text analysis and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards.

### 1a. Load Libraries

Before introducing the {vader} package, let's load some two familiar packages that we can also use for performing sentiment analysis:

```{r}
library(tidyverse)
library(tidytext)
```

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment Reasoning (VADER), a rule-based model for general sentiment analysis of social media text and specifically attuned to measuring sentiment in microblog-like contexts.

To learn more about the {vader} package and its development, take a look at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious Rule-based Model forSentiment Analysis of Social Media Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the vader library:

```{r}
library(vader)
```

**Note:** The {vader} package can take quite some time to run on a large dataset like ours so it's included in our reach section for you to explore at your leisure. The example show in the reach section includes just a small subset of tweets for demonstration purposes.

### 1b. Import Data

Finally, let's import our `ss_tidy_tweets.csv` data from Learning Lab 1 that we'll be using for our sentiment analysis:

```{r}
ss_tidy_tweets <- read_csv("data/ss_tidy_tweets.csv", 
                   col_types = cols(author_id = col_character(),
                                    conversation_id = col_character(), 
                                    id = col_character()))
```

#### [Your Turn]{style="color: green;"} â¤µ

Use the head, tail, glimpse or other view function or method to quickly inspect our data and make sure everything looks in order:

```{r}
# your code here
```

------------------------------------------------------------------------

## 2. WRANGLE

Since we're working tweets that have already been tidied in Learning Lab 1, for this learning lab our data wrangling skills will focus on how to:

a.  **Get Sentiments**. We introduce sentiment lexicons, take a look at how each lexicon assigns sentiment, and learn a little bit more about how each was validated.

b.  **Join Sentiment.** We learn yet another join function, the `inner_join()`, for appending sentiment values to our data frame.

For a quick overview of the different join functions with helpful visuals, here is an additional resource to the one shared in Learning Lab 1: [\<https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\>](https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti){.uri}

### 2a. Get Sentiments

Recall from our readings that sentiment analysis tries to evaluate words for their emotional association. Silge & Robinson point out that, "one way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words." This isn't the only way to approach sentiment analysis, but it is an easier entry point into sentiment analysis and one that often-used.

The {tidytext} package introduced in Learning Lab 1 provides access to several sentiment lexicons based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth.

The three general-purpose lexicons we'll focus on are:

-   `AFINN` assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

-   `bing` categorizes words in a binary fashion into positive and negative categories.

-   `nrc`Â categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

Note that if this is your first time using the AFINN and NRC lexicons and you are working in RStudio Desktop, you'll be prompted to download both. Respond yes to the prompt by entering "1" and the NRC and AFINN lexicons will download. You'll only have to do this the first time you use the NRC lexicon.

Let's take a quick look at each of these lexicons using the `get_sentiments()` function and assign them to their respective names for later use:

```{r}
afinn <- get_sentiments("afinn")

afinn
```

```{r}
bing <- get_sentiments("bing")

bing
```

```{r}
nrc <- get_sentiments("nrc")

nrc
```

And just out of curiosity, let's take a look at the `loughran` lexicon as well:

```{r}
loughran <- get_sentiments("loughran")

loughran
```

#### [Your Turn]{style="color: green;"} â¤µ

In the space below, answer the following questions:

1.  How were these sentiment lexicons put together and validated? Hint: take a look at [Chapter 2 from Text Mining with R](https://www.tidytextmining.com/sentiment.html#the-sentiments-datasets).

    -   

2.  Why do you think we should be cautious when using and interpreting these lexicons?

    -   

#### Join Sentiments

We've reached the final step in our data wrangling process before we can begin exploring our data to address our research questions.

In the previous section, we usedÂ `anti_join()`Â to remove stop words in our dataset. For sentiment analysis, we're going use the `inner_join()` function to do something similar.

Unlike the `anti_join()` function that removes rows that contain words matching those in our stop words dictionary, `inner_join()` allows us to keep only the rows with words that match words in our sentiment lexicons, or dictionaries, along with the sentiment measure for that word from the sentiment lexicon.

Let's use `inner_join()` to combine our two `tidy_tweets` and `afinn` data frames, keeping only rows with matching data in the `word` column:

```{r}
sentiment_afinn <- inner_join(ss_tidy_tweets, afinn, by = "word")

sentiment_afinn
```

Notice that each word in your `sentiment_afinn` data frame now contains a value ranging from -5 (very negative) to 5 (very positive).

Let's join and take a look at the our sentiments according to bing:

```{r}
sentiment_bing <- inner_join(ss_tidy_tweets, bing, by = "word")

sentiment_bing
```

#### [Your Turn]{style="color: green;"} â¤µ

1.  What are three words in our our `sentiments_afinn` data frame are the most that are most negative? Positive?

    -   

Use the code chunk below to create a `sentiment_nrc` data frame and answer the following questions:

```{r}
# your code here
```

1.  What do you notice about the change in the number of observations (i.e. words) between the `ss_tidy_tweets` and data frames with sentiment values attached? Why did this happen?

    -   

## 3. EXPLORE

Now that we have our tweets tidied and sentiments joined, we're ready for a little data exploration. As highlighted in Learning Labs 1 & 2, calculating summary statistics and data visualization are a key part of exploring text. One goal in this phase is explore questions that drove the original analysis. Topics addressed in Section 3 include:

a.  **Count Sentiments**. Similar to Learning Lab 1, we put together some basic counts of the sentiment assigned to words in our tweets.
b.  **Create Summaries**. In part 2 of Explore, summaries of our sentiment values in order to compare public sentiment between NGSS and CCSS

### 3a. Count Sentiments

Since our primary goals is to compare public sentiment around the NGSS and CCSS state standards, in this section we put together some basic numerical summaries using our familiar tidyverse functions and the lexicons introduce above to see whether tweets are generally more positive or negative for each standard as well as differences between the two.

Let's start with `bing`, our simplest sentiment lexicon, and use the `count` function to count how many times in our `sentiment_bing` data frame "positive" and "negative" occur in `sentiment` column and :

```{r}
summary_bing <- count(sentiment_bing, sentiment, sort = TRUE)

summary_bing
```

Collectively, it looks like our combined dataset has more negative words than positive words.

#### [Your Turn]{style="color: green;"} â¤µ

Since our main goal is to compare positive and negative sentiment between CCSS and NGSS, add the `group_by()` function and supply the appropriate argument again to get `sentiment` summaries for NGSS and CCSS separately:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>%
  count(sentiment) 

summary_bing
```

Looks like CCSS have far more negative words than positive, while NGSS skews much more positive. So far, pretty consistent with the findings in our guiding study!

### 3b. Create Sentiment Summaries

Our last step will be calculate a single sentiment "score" for our tweets that we can use for quick comparison and create a new variable indicating which lexicon we used.

First, let's untidy our data a little by introducing the `spread` function from the `tidyr` package to transform our `sentiment` column into separate columns for `negative` and `positive` that contains the `n` counts for each:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) 

summary_bing
```

Finally, we'll use the `mutate` function to create two new variables: `sentiment` and `lexicon` so we have a single sentiment score and the lexicon from which it was derived:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(lexicon = "bing") %>%
  relocate(lexicon)

summary_bing
```

There we go, now we can see that CCSS scores negative, while NGSS is overall positive. By quite a bit in fact!!

Let's calculate a quick score using the `afinn` lexicon now. Remember that AFINN provides a value from -5 to 5 for each:

```{r}
head(sentiment_afinn)
```

To calculate a summary score, we will need to first group our data by `standards` again and then use the `summarise` function to create a new `sentiment` variable by adding all the positive and negative scores in the `value` column:

```{r}
summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```

Again, CCSS is overall negative while NGSS is overall positive!

#### [Your Turn]{style="color: green;"} â¤µ

Now it your turn to calculate a single sentiment score for NGSS and CCSS using either the `loughan` or `nrc` lexicon. Just like the `bing` lexicon, the `loughan` lexicon only scores words as "positive" or "negative."

The `nrc` lexicon, however, contains "positive" and "negative" values just like `bing` and `loughan`, but also includes values like "trust" and "sadness" as shown below. You will need to use the `filter()` function as an additional step to select rows that only contain "positive" and "negative" values.

```{r}
# your code here
```

1.  Are these findings above still consistent with those from other lexicons above?

    -   

## 4. MODEL

Recall from the PREPARE section that the Rosenberg et al. study was guide by the following questions:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for teachers differ from non-teachers?
3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?
4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?
5.  How does public sentiment vary over time?

Similar to our sentiment summary using the AFINN lexicon, the Rosenberg et al. study used the -5 to 5 sentiment score from the SentiStrength lexicon to answer RQ \#1. To address the remaining questions the authors used a mixed effects model (also known as multi-level or hierarchical linear models via the lme4 package in R.

Collectively, the authors found that:

1.  The SentiStrength scale indicated an overall neutral sentiment for tweets about the Next Generation Science Standards.
2.  Teachers were more positive in their posts than other participants.
3.  Posts including \#NGSSchat that were posted outside of chats were slightly more positive relative to those that did not include the \#NGSSchat hashtag.
4.  The effect upon individuals of being involved in the \#NGSSchat was positive, suggesting that there is an impact on individuals---not tweets---of participating in a community focused on the NGSS.
5.  Posts about the NGSS became substantially more positive over time.

## 5. COMMUNICATE

Congratulations - you've made it to the end! To complete your work, you can click the drop down arrow at the top of the file, then select "Knit top HTML". This will create a report in your Files pane that serves as a record of your code and its output you can open or share.

## R-each (Optional)

R you ready to Come to the Dark Side?!?! If yes, then try our R-each!

![](img/dark-side.jpeg){width="80%"}

If you're using data that you brought to the institute or data that you pulled from Twitter, try computing sentiment values for your own data.

1.  we learning how to use the {vader} package to assign sentiment scores the each tweet, rather than lumping scores for each tweet into a single "bag of words."

If you'd like to use the data we've been working with for your reach,

```{r}
ccss_sample <- ss_tweets %>% 
  filter(standards == "ccss") %>%
  sample_n(500)

ccss_sample
```

```{r}
vader_summary <- vader_df(ss_sample$text)

mean(vader_summary$compound)
```

## Reflection

In this learning lab, we focused we focus on the use of lexicons for text analysis and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards. Below, add a few notes in response to the following prompts:

1.  One thing I took away from this learning lab:

    -   

2.  One thing I want to learn more about:

    -   
