---
title: "TM Lab 1: Tidy Text, Tokens & Twitter"
author: "LASER TEAM"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[INSERT PRESENTATION VIDEO HERE]

## 1. PREPARE

Recall from our presentation that one central question to text mining and natural language processing is:

> How do we to **quantify** what a document or collection of documents is about?

For our first lab on text mining in STEM education research, we'll explore this question by examining a corpus, or collection, of public posts on Twitter about the Common Core State Standards (CCSS) to better understand public discourse surrounding these standards, particularly as they relate to math education. Specifically, in this lab we'll be applying some basic text mining techniques to address the following questions:

1.  Nn

2.  gg

To help us better understand the packages, Twitter API tools, and data we'll use during this lab to address these questions, in this section we'll learn to:

a.  **Load Packages** for tidy text mining and using Twitter APIs

b.  **Create a Twitter App** to obtain [authentication](https://developer.twitter.com/en/docs/authentication) credentials, also known as keys and tokens

c.  **Authorize RStudio** to use your app for retrieving data from Twitter

#### Exercise RMarkdown File

Although I highly recommend that that you manually type the code shared throughout this Learning Lab, for large blocks of text it may be easier to copy and paste.

### 1a. Load Packages

#### Prior Packages

Let's begin by loading some familiar packages from previous Learning Labs:

```{r load-libraries, message=FALSE}
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(readxl)
library(writexl)
```

#### New Packages

![](img/tidytext.png){width="100"}

The `tidytext` package provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages.

Let's go ahead and load it:

```{r load-tidytext, message=FALSE}
library(tidytext)
```

As we'll Learn first hand later in this lab, using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Much of the infrastructure needed for text mining with tidy data frames already exists in `tidyverse` packages with which we've already been introduced.

![](img/rtweet.jpeg){width="100"}

The `rtweet` package provides users a range of functions designed to extract data from Twitter's REST and streaming APIs and has three main goals:

1.  Formulate and send requests to Twitter's REST and stream APIs.

2.  Retrieve and iterate over returned data.

3.  Wrangling data into tidy structures.

Let's load the `rtweet` package which we'll be using later in this lab to accomplish all three of the goals listed above:

```{r load-rtweet, message=FALSE}
library(rtweet)
```

To learn more about the `rtweet` package, you can find full documentation on CRAN at: <https://cran.r-project.org/web/packages/rtweet/rtweet.pdf>

And for a quick demo of the functions, check out the package vignette by running the following code in your console:

```{r vignette-rtweet, eval=FALSE}
vignette("rtweet")
```

### 1b. Create a Twitter App

Before you can begin pulling tweets into R, you'll first need to create a Twitter App in your developer account. This section and the section that follows, are borrowed largely from the `rtweet` package by Michael Kearney, and requires that you have set up a Twitter developer account.

You are not required to set up developer account for this institute, but if you are still interested in creating one, [these instructions](https://dev.to/sumedhpatkar/beginners-guide-how-to-apply-for-a-twitter-developer-account-1kh7) succinctly outline the process and you can set one up in about 10 minutes. We have provided the data we'll be using for the Wrangling and Explore parts of the lab on our [GitHub repository](https://github.com/sbkellogg/eci-588/tree/main/unit-2/data) and you can skip to section [2b. Tidy Text] if you are not interested in or unable to set up a Twitter developer account.

#### Steps for Creating your Twitter App

1.  Navigate to [developer.twitter.com/en/apps](https://developer.twitter.com/en/apps/), click the blue button that says, `Create a New App`, and then complete the form with the following fields:

    -   `App Name`: What your app will be called

    -   `Application Description`: How your app will be described to its users

        <p align="center">

        <img src="files/create-app-1.png" alt="create-app-1"/>

        </p>

    -   `Website URLs`: Website associated with app--I recommend using the URL to your Twitter profile

    -   `Callback URLs`: ***IMPORTANT*** enter exactly the following: `http://127.0.0.1:1410`

        <p align="center">

        <img src="files/create-app-2.png" alt="create-app-2"/>

        </p>

    -   `Tell us how this app will be used`: Be clear and honest

        <p align="center">

        <img src="files/create-app-3.png" alt="create-app-3"/>

        </p>

2.  When you've completed the required form fields, click the blue `Create` button at the bottom

3.  Read through and indicate whether you accept the developer terms

    <p align="center">

    <img src="files/create-app-4.png" alt="create-app-4"/>

    </p>

4.  And you're done!

    <p align="center">

    <img src="files/create-app-5.png" alt="create-app-5"/>

    </p>

### 1c. Authorizing RStudio

In order to authorize R to use your Twitter App to retrieve data, you'll need to create a personal Twitter token by completing the following steps:

-   Navigate to [developer.twitter.com/en/apps](https://developer.twitter.com/en/apps) and select your Twitter app
-   Click the tab labeled `Keys and tokens` to retrieve your keys.
-   Locate the `Consumer API keys` (aka "API Secret").

<p align="center">

<img src="files/create-app-6.png" alt="create-app-6"/>

</p>

-   Scroll down to `Access token & access token secret` and click `Create`

<p align="center">

<img src="files/create-app-7.png" alt="create-app-7"/>

</p>

-   Copy and paste the four keys (along with the name of your app) into an R script file and pass them along to `create_token()`. Note, these keys are named secret for a reason. I recommend setting up your token in a separate R script than the one that you will eventually share.

```{r api-keys, eval=FALSE}
## store api keys (these are fake example values; replace with your own keys)
app_name <- "Text Mining in Education"
api_key <- "afYS4vbIlPAj096E60c4W1fiK"
api_secret_key <- "bI91kqnqFoNCrZFbsjAWHD4gJ91LQAhdCJXCj3yscfuULtNkuu"
access_token <- "9551451262-wK2EmA942kxZYIwa5LMKZoQA4Xc2uyIiEwu2YXL"
access_token_secret <- "9vpiSGKg1fIPQtxc5d5ESiFlZQpfbknEN1f1m2xe5byw7"

## authenticate via web browser
token <- create_token(
  app = app_name,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

If you are interested in viewing an alternate authentication method, you can view `rtweet` Twitter authorization vignette by running the following code:

```{r rtweet-vignette, eval=FALSE}
vignette("auth")
```

#### Authorization in future R sessions

-   The `create_token()` function should automatically save your token as an environment variable for you. So next time you start an R session [on the same machine], rtweet should automatically find your token.
-   To make sure it works, restart your R session, run the following code, and again check to make sure the app name and `api_key` match.

```{r get-token}
## check to see if the token is loaded
get_token()
```

That's it!

------------------------------------------------------------------------

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled (Krumm et al, 2018).

a.  **Import Data**. In this section, we introduce new functions from the `rtweet` package to search for tweets and users of interest.
b.  **Tidy Tweets**. We also introduce the `tidytext` package to both "tidy" and tokenize our tweets in order to create our data frame for analysis.
c.  **Remove Stop Words.** We conclude our data wrangling by using the now familiar `dplyr` package to remove words that don't add much value to our analysis.

### 2a. Import Tweets

This section introduces the following functions from the `rtweet` package for reading Twitter data into R:

-   [`search_tweets()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets) Pulls up to 18,000 tweets from the last 6-9 days matching provided search terms. 
-   [`search_tweets2()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets) Returns data from **multiple** search queries.
-   [`get_timelines()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets) Returns up to 3,200 tweets of one or more specified Twitter users.

#### Search Tweets

Since one of our goals for this Learning Lab and the next is a very crude replication of the studies by Wang and Fikis (2019) and by Rosenberg et al. (2021), let's begin by introducing the `search_tweets()` function to try reading into R 5,000 tweets containing the commoncore hashtag and store as a new data frame `ccss_tweets`.

Type or copy the following code into your R script or console and run:

```{r search-tweets, eval=TRUE}
ccss_tweets <- search_tweets(q = "#commoncore", n=5000)
```

Note that the first argument `q =` that the `search_tweets()` function expects is the search term included in quotation marks and that `n =` specifies the maximum number of tweets

##### ✅ Comprehension Check

View your new `ccss_tweets` data frame using the `glimpse()` function introduced previously to help answer the following questions:

1.  How many tweets did our query using the Twitter API actually return? How many variables?
2.  Why do you think our query pulled in far less than 5,000 tweets requested?
3.  Does our query also include retweets? How do you know?

```{r glimpse-tweets}
glimpse(ccss_tweets)
```

#### Remove Retweets

While not explicitly mentioned in the paper, it's likely the authors removed retweets in their query since a retweet is simply someone else reposting someone else's tweet and would duplicate the exact same content of the original.

Let's use the `include_rts =` argument to remove any retweets by setting it to `FALSE`:

```{r retweet-argument, eval=TRUE}
ccss_tweets <- search_tweets("commoncore", 
                                   n=5000, 
                                   include_rts = FALSE)

head(ccss_tweets)
```

#### Using the OR Operator

If you recall from [Section 1a], the authors accessed tweets and user information from the hashtag-based \#NGSSchat online community, all tweets that included any of the following phrases, with "/" indicating an additional phrase featuring the respective plural form: "ngss", "next generation science standard/s", "next gen science standard/s".

Let's modify our query using the `OR` operator to also include "ngss" so it will return tweets containing either \#NGSSchat or "ngss" and assign to `ngss_or_tweets`:

```{r or-operator}
ccss_or_tweets <- search_tweets(q = "#commoncore OR commoncore", 
                                n=5000,
                                include_rts = FALSE)

```

##### ✅ Comprehension Check

Try including both search terms but excluding the `OR` operator to answer the following question:

1.  Does excluding the `OR` operator return more tweets, the same number of tweets, or fewer tweets? Why?
2.  What other useful arguments does the `search_tweet()` function contain? Try adding one and see what happens.

Hint: Use the `?search_tweets` help function to learn more about the `q` argument and other arguments for composing search queries.

#### Use Multiple Queries

Unfortunately, the `OR` operator will only get us so far. In order to include the additional search terms, we will need to use the `c()` function to combine our search terms into a single list.

The `rtweets` package has an additional `search_tweets2()` function for using multiple queries in a search. To do this, either wrap single quotes around a search query using double quotes, e.g., `q = '"next gen science standard"'` or escape each internal double quote with a single backslash, e.g., `q = "\"next gen science standard\""`.

Copy and past the following code to store the results of our query in `ngss_tweets`:

```{r}
ngss_tweets <- search_tweets2(c("#NGSSchat OR ngss",
                                '"next generation science standard"',
                                '"next generation science standards"',
                                '"next gen science standard"',
                                '"next gen science standards"'
                                   ), 
                             n=5000,
                             include_rts = FALSE)
```

#### Our First Dictionary

Recall that for our research question we wanted to compare public sentiment about both the NGSS and CCSS state standards. Let's go ahead and create our very first "dictionary" for identifying tweets related to either set of standards, and then use that dictionary for our the `q =` query argument to pull tweets related to the state standards.

To do so, we'll need to add some additional search terms to our list:

```{r}
ccss_dictionary <- c("#commoncore", '"common core"')

ccss_tweets <- ccss_dictionary %>% 
  search_tweets2(n=5000, include_rts = FALSE)
```

Notice that you can use the pipe operator with the `search_tweets()` function just like you would other functions from the tidyverse.

##### ✅ Comprehension Check

1.  Use the `search_tweets` function to create you own custom query for a twitter hashtag or topic(s) of interest.

#### Write to Excel

Finally, let's save our tweet files to use in later exercises since tweets have a tendency to change every minute. We'll save as a Microsoft Excel file since one of our columns can not be stored in a flat file like .csv.

Let's use the `write_xlsx()` function from the `writexl` package just like we would the `write_csv()` function from `dplyr` in Unit 1:

```{r, eval=F}
write_xlsx(ccss_tweets, "data/csss_tweets.xlsx")
```

#### Other Useful Queries

For your independent analysis, you may be interest in exploring posts by specific users rather than topics, key words, or hashtags. Yes, there is a function for that too!

For example, let's create another list containing the usernames of me and some of my colleagues at the Friday Institute using the `c()` function again and use the `get_timelines()` function to get the most recent tweets from each of those users:

```{r}
fi <- c("sbkellogg", "mjsamberg", "haspires", "tarheel93", "drcallie_tweets", "AlexDreier")

fi_tweets <- fi %>%
  get_timelines(include_rts=FALSE)
```

And let's use the `sample_n()` function from the `dplyr` package to pick 10 random tweets and use `select()` to select and view just the `screenname` and `text` columns that contains the user and the content of their post:

```{r}
sample_n(fi_tweets, 10) %>%
  select(screen_name, text)
```

We've only scratched the surface of the number of functions available in the `rtweets` package for searching Twitter. Use the following function to

```{r, eval=F}
vignette("intro", package="rtweet")
```

##### ✅ Comprehension Check

To conclude Section 2a, try one of the following search functions from the `rtweet` vignette:

1.  `get_timelines()` Get the most recent 3,200 tweets from users.
2.  `stream_tweets()` Randomly sample (approximately 1%) from the live stream of all tweets.
3.  `get_friends()` Retrieve a list of all the accounts a user follows.
4.  `get_followers()` Retrieve a list of the accounts following a user.
5.  `get_favorites()` Get the most recently favorited statuses by a user.
6.  `get_trends()` Discover what's currently trending in a city.
7.  `search_users()` Search for 1,000 users with the specific hashtag in their profile bios.

### 2b. Tidy Text

Now that we have the data needed to address our questions, we still have a little bit of work to do to get it ready for analysis. This section will revisit some familiar functions from Unit 1 and introduce a couple new functions:

#### Functions Used

**`dplyr` functions**

-   `select()` picks variables based on their names.
-   `slice()` lets you select, remove, and duplicate rows.
-   `rename()` changes the names of individual variables using new_name = old_name syntax
-   `filter()` picks cases, or rows, based on their values in a specified column.

**`tidytext` functions**

-   `unnest_tokens()` splits a column into tokens
-   `anti_join()` returns all rows from x with**out** a match in y.

**ATTENTION:** For those of you who do not have Twitter Developer accounts, you will need to read in the Excel files share in our Course site and also located here: <https://github.com/sbkellogg/eci-588/tree/main/unit-2/data>

We'll use the `readxl` package highlighted in Unit 1 and the `read_xlsx()` function to read in the data stored in the data folder of our R project:

```{r}
ccss_tweets <- read_xlsx("data/csss_tweets.xlsx")
```

**Note:** If you have already created these data frames from 2a. Import Tweets, you do not need to read these file into R unless you want to reproduce the exact same outputs shown in the rest of this Learning Lab.

#### Subset Rows & Columns

As you are probably already aware, we have way more data than we'll need for analysis and will need to pare it down quite a bit.

First, let's use the `filter` function to subset rows containing only tweets in the language:

```{r}
ngss_text <- filter(ngss_tweets, lang == "en")
```

Now let's select the following columns from our new `ngss_text` data frame:

1.  `screen_name` of the user who created the tweet
2.  `created_at` timestamp for examining changes in sentiment over time
3.  `text` containing the tweet which is our primary data source of interestt

```{r}
ngss_text <- select(ngss_text,screen_name, created_at, text)
```

#### Add & Reorder Columns

Since we are interested in comparing the sentiment of NGSS tweets with CSSS tweets, it would be helpful if we had a column for quickly identifying the set of state standards, with which each tweet is associated.

We'll use the `mutate()` function to create a new variable called `standards` to label each tweets as "ngss":

```{r}
ngss_text <- mutate(ngss_text, standards = "ngss")
```

And just because it bothers me, I'm going to use the `relocate()` function to move the `standards` column to the first position so I can quickly see which standards the tweet is from:

```{r}
ngss_text <- relocate(ngss_text, standards)
```

Note that you could also have used the `select()` function to reorder columns like so:

```{r}
ngss_text <- select(ngss_text, standards, screen_name, created_at, text)
```

Finally, let's rewrite the code above using the `%>%` operator so there is less redundancy and it is easier to read:

```{r}
ngss_text <-
  ngss_tweets %>%
  filter(lang == "en") %>%
  select(screen_name, created_at, text) %>%
  mutate(standards = "ngss") %>%
  relocate(standards)
```

##### ✅ Comprehension Check

**WARNING:** You will not be able to progress to the next section until you have completed the following task:

1.  Create an new `ccss_text` data frame for our `ccss_tweets` Common Core tweets by modifying code above.

```{r, echo=F}
ccss_text <-
  ccss_tweets %>%
  filter(lang == "en") %>%
  select(screen_name, created_at, text) %>%
  mutate(standards = "ccss") %>%
  relocate(standards)
```

#### Combine Data Frames

Finally, let's combine our `ccss_text` and `ngss_text` into a single data frame by using the `bind_rows()` function from `dplyr` to simply supplying the data frames that you want to combine as arguments:

```{r}
tweets <- bind_rows(ngss_text, ccss_text)
```

And let's take a quick look at both the `head()` and the `tail()` of this new `tweets` data frame to make sure it contains both "ngss" and "ccss" standards:

```{r}
head(tweets)
tail(tweets)
```

#### Tokenize Text

We have a couple remaining steps to tidy our text that hopefully should feel familiar by this point. If you recall from [Chapter 1 of Text Mining With R](https://www.tidytextmining.com/tidytext.html), Silge & Robinson describe **tokens** as:

> A meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.

First, let's tokenize our tweets by using the `unnest_tokens()` function to split each tweet into a single row to make it easier to analyze:

```{r}
tweet_tokens <- 
  tweets %>%
  unnest_tokens(output = word, 
                input = text, 
                token = "tweets")
```

Notice that we've included an additional argument in the call to `unnest_tokens()`. Specifically, we used the specialized `“tweets”` tokenizer in the `tokens =` argument that is very useful for dealing with Twitter text or other text from online forums in that it retains hashtags and mentions of usernames with the \@ symbol.

### 2c. Remove Stop Words

Now let's remove stop words like "the" and "a" that don't help us learn much about what people are tweeting about the state standards.

```{r}
tidy_tweets <-
  tweet_tokens %>%
  anti_join(stop_words, by = "word")
```

Notice that we've specified the `by =` argument to look for matching words in the `word` column for both data sets and remove any rows from the `tweet_tokens` dataset that match the `stop_words` dataset. Remember when we first tokenized our dataset I conveniently chose `output = word` as the column name because it matches the column name `word` in the `stop_words` dataset contained in the `tidytext` package. This makes our call to `anti_join()`simpler because `anti_join()` knows to look for the column named `word` in each dataset. However this wasn't really necessary since `word` is the only matching column name in both datasets and it would have matched those columns by default.

#### Custom Stop Words

Before wrapping up, let's take a quick count of the most common words in `tidy_tweets` data frame:

```{r}
count(tidy_tweets, word, sort = T)
```

Notice that the nonsense word "amp" is in our top tens words. If we use the `filter()` function and \`grep() query from Unit 1 on our `tweets` data frame, we can see that "amp" seems to be some sort of html residue that we might want to get rid of.

```{r}
filter(tweets, grepl('amp', text))
```

Let's rewrite our stop word code to add a custom stop word to filter out rows with "amp" in them:

```{r}
tidy_tweets <-
  tweet_tokens %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word == "amp")
```

Note that we could extend this filter to weed out any additional words that don't carry much meaning but skew our data by being so prominent.

##### ✅ Comprehension Check

We've created some unnecessarily lengthy code to demonstrate some of the steps in the tidying process. Rewrite the tokenization and removal of stop words processes into a more compact series of commands and save your data frame as `tidy_tweets`.

## 3. EXPLORE

Now that we have our tweets tidied and sentiments joined, we're ready for a little data exploration. As highlighted in Unit 1, calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. One goal in this phase is explore questions that drove the original analysis and develop new questions and hypotheses to test in later stages. Topics addressed in Section 3 include:

a.  **Time Series**. We take a quick look at the date range of our tweets and compare number of postings by standards.
b.  

### 3a. Time Series

Before we dig into sentiment, let's use the handy `ts_plot` function built into `rtweet` to take a very quick look at how far back our tidied `tweets` data set goes:

```{r}
ts_plot(tweets, by = "days")
```

Notice that this effectively creates a `ggplot` time series plot for us. I've included the `by =` argument which by default is set to "days". It looks like tweets go back 9 days which the rate limit set by Twitter.

Try changing it to "hours" and see what happens.

##### ✅ Comprehension Check

1.  Use `ts_plot` with the `group_by` function to compare the number of tweets over time by Next Gen and Common Core `standards`
2.  Which set of standards is Twitter users talking about the most?

Hint: use the `?ts_plot` help function to check the examples to see how this can be done.

Your line graph should look something like this:

```{r, echo=F}
tweets %>%
  group_by(standards) %>%
  ts_plot(by = "days")
```
