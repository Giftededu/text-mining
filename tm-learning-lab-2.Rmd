---
title: "TM Lab 1a: Twitter API "
author: "LASER TEAM"
date: "2/21/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 3. EXPLORE

As highlighted in DSEIUR and Learning Analytics Goes to School, calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. In Section 3, we will calculate some very basic summary statistics from our tidied text, explore key words of interest to gather additional context, and use data visualization to identify patterns and trends that may not be obvious from our tables and numerical summaries. Topics addressed in Section 3 include::

a.  **c**

b.  **Time Series**. We take a quick look at the date range of our tweets and compare number of postings by standards.

### 3a. Count Words

> People new to text mining are often disillusioned when they figure out how it's actually done --- which is still, in large part, by counting words. They're willing to believe that computers have developed some clever strategy for finding patterns in language --- but think "surely it's something better than *that?*"

The quote above from [Word Counts are Amazing](https://tedunderwood.com/2013/02/20/wordcounts-are-amazing/) by Ted Underwood...

Recall from the previous section that one overarching question guiding most of our efforts in these text mining labs is: "How do we **quantify** what a text is about?"

\

```{r}
 library(wordcloud2)

tidy_unigrams %>%
  count(word) %>%
  filter(n > 200) %>%
  wordcloud2()
```

```{r}

tidy_unigrams %>%
  count(word) %>%
  filter(n > 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col()

```

\

```{r}
ccss_tweets %>%
  select(text) %>% 
  filter(grepl('math', text)) %>%
  sample_n(20)
```

\

```{r}

unigram_counts <- tidy_unigrams %>%
  count(word) %>%
  filter(n > 200)

wordcloud2(unigram_counts,
           color = ifelse(unigram_counts[, 2] > 800, 'black', 'gray'))
```

\

##  Bigrams

In the function above we specified tokens as individual words, but many interesting text analyses are based on the relationships between words, which words tend to follow others immediately, or words that tend to co-occur within the same documents.

We can also use the `unnest_tokens()` function to tokenize our tweets into consecutive sequences of words, called **n-grams**. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them as well see in Part 2.

We do this by adding the `token = "ngrams"` option to [`unnest_tokens()`](https://rdrr.io/pkg/tidytext/man/unnest_tokens.html), and setting `n` to the number of words in each n-gram. Let's set `n` to 2, so we can examine pairs of two consecutive words, often called "bigrams":

```{r ccss-bigrams}
ss_tweets_b1 <- ss_tweets %>% 
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)

head(ss_tweets_b1)
```

#### Filtering Bigrams

As we saw above, a lot of the most common bigrams are pairs of common (uninteresting) words as well. Dealing with these is a little less straightforward and we'll need to use the `separate()` function from the `tidyr` package, which splits a column into multiple based on a delimiter. This lets us separate it into two columns, "word1" and "word2", at which point we can remove cases where either is a stop-word.

```{r stop-bigrams}
library(tidyr)

ss_tweets_b2 <- ss_tweets_b1 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

ss_tweets_b3 <- ss_tweets_b2 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

ss_tweets_b4 <- ss_tweets_b3 %>%
  unite(bigram, word1, word2, sep = " ")
```

\

### 3b. Graph Pairs

```{r}
tidy_bigrams %>%
  count(bigram, sort = TRUE)

```

\

```{r}

library(igraph)
library(ggraph)
library(stringr)

math_bigrams <- ccss_tweets %>%
  filter(str_detect(text, 'math')) %>%
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)

bigrams_separated <- math_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_graph <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE) %>%
  filter(n > 10) %>%
  graph_from_data_frame()


set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n)) +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

\

\

```{r}

```

## 4. MODEL {data-link="4. MODEL"}

```{r}

library(widyr)

math_unigrams <- ccss_tweets %>%
  filter(str_detect(text, 'math')) %>%
  unnest_tokens(word, 
                text, 
                token = "tweets")

word_pairs <- math_unigrams %>%
  pairwise_count(word, id, sort = TRUE)
```

\

```{r}

word_cors <- math_unigrams %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, id, sort = TRUE)

word_cors %>%
  filter(item1 == "math")

```

\

```{r}
word_cors <- tidy_unigrams %>%
  group_by(word) %>%
  filter(n() >= 50) %>%
  pairwise_cor(word, id, sort = TRUE)
```

\

```{r}
word_cors %>%
  filter(item1 %in% c("math", "#math")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

\

```{r}
word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

## 3. EXPLORE

Now that we have our tweets tidied and sentiments joined, we're ready for a little data exploration. As highlighted in Unit 1, calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. One goal in this phase is explore questions that drove the original analysis and develop new questions and hypotheses to test in later stages. Topics addressed in Section 3 include:

a.  **Time Series**. We take a quick look at the date range of our tweets and compare number of postings by standards.
b.  **Sentiment Summaries**. We put together some basic summaries of our sentiment values in order to compare public sentiment

### 3a. Time Series

Before we dig into sentiment, let's use the handy `ts_plot` function built into `rtweet` to take a very quick look at how far back our tidied `tweets` data set goes:

```{r}
ts_plot(tweets, by = "days")
```

Notice that this effectively creates a `ggplot` time series plot for us. I've included the `by =` argument which by default is set to "days". It looks like tweets go back 9 days which the rate limit set by Twitter.

Try changing it to "hours" and see what happens.

##### ✅ Comprehension Check

1.  Use `ts_plot` with the `group_by` function to compare the number of tweets over time by Next Gen and Common Core `standards`
2.  Which set of standards is Twitter users talking about the most?

Hint: use the `?ts_plot` help function to check the examples to see how this can be done.

Your line graph should look something like this:

```{r, echo=F}
tweets %>%
  group_by(standards) %>%
  ts_plot(by = "days")
```

### 3b. Sentiment Summaries

Since our primary goals is to compare public sentiment around the NGSS and CCSS state standards, in this section we put together some basic numerical summaries using our different lexicons to see whether tweets are generally more positive or negative for each standard as well as differences between the two. To do this, we revisit the following `dplyr` functions:

-   [`count()`](https://dplyr.tidyverse.org/reference/count.html?q=count) lets you quickly count the unique values of one or more variables

-   [`group_by()`](https://dplyr.tidyverse.org/articles/grouping.html?q=group) takes a data frame and one or more variables to group by

-   [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) creates a numerical summary of data using arguments like [`mean()`](https://rdrr.io/r/base/mean.html) and [`median()`](https://rdrr.io/r/stats/median.html)

-   [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) adds new variables and preserves existing ones

And introduce one new function:

-   `spread()`

#### Sentiment Counts

Let's start with `bing`, our simplest sentiment lexicon, and use the `count` function to count how many times in our `sentiment_bing` data frame "positive" and "negative" occur in `sentiment` column and :

```{r}
summary_bing <- count(sentiment_bing, sentiment, sort = TRUE)
```

Collectively, it looks like our combined dataset has more positive words than negative words.

```{r}
summary_bing
```

Since our main goal is to compare positive and negative sentiment between CCSS and NGSS, let's use the `group_by` function again to get `sentiment` summaries for NGSS and CCSS separately:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment) 

summary_bing
```

Looks like CCSS have far more negative words than positive, while NGSS skews much more positive. So far, pretty consistent with Rosenberg et al. findings!!!

#### Compute Sentiment Value

Our last step will be calculate a single sentiment "score" for our tweets that we can use for quick comparison and create a new variable indicating which lexicon we used.

First, let's untidy our data a little by using the `spread` function from the `tidyr` package to transform our `sentiment` column into separate columns for `negative` and `positive` that contains the `n` counts for each:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) 

summary_bing
```

Finally, we'll use the `mutate` function to create two new variables: `sentiment` and `lexicon` so we have a single sentiment score and the lexicon from which it was derived:

```{r}
summary_bing <- sentiment_bing %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(lexicon = "bing") %>%
  relocate(lexicon)

summary_bing
```

There we go, now we can see that CCSS scores negative, while NGSS is overall positive.

Let's calculate a quick score for using the `afinn` lexicon now. Remember that AFINN provides a value from -5 to 5 for each:

```{r}
head(sentiment_afinn)
```

To calculate late a summary score, we will need to first group our data by `standards` again and then use the `summarise` function to create a new `sentiment` variable by adding all the positive and negative scores in the `value` column:

```{r}
summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```

Again, CCSS is overall negative while NGSS is overall positive!

##### ✅ Comprehension Check

For your final task for this walkthough, calculate a single sentiment score for NGSS and CCSS using the remaining `nrc` and `loughan` lexicons and answer the following questions. Are these findings above still consistent?

Hint: The `nrc` lexicon contains "positive" and "negative" values just like `bing` and `loughan`, but also includes values like "trust" and "sadness" as shown below. You will need to use the `filter()` function to select rows that only contain "positive" and "negative."

```{r}
nrc
```

```{r, echo=F, message=F}
summary_nrc <- sentiment_nrc %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "nrc")  %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive/negative)

summary_nrc

summary_afinn <- sentiment_afinn %>% 
  group_by(standards) %>% 
  summarise(sentiment = sum(value)) %>%
  mutate(lexicon = "AFINN") %>%
  relocate(lexicon)

summary_afinn
```
